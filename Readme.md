# Poetry ClassificationSome issues unrelevant to code itself will be ignored. These parts will be completed at the end.## Code Logic1. preprocess.py- Before preprocessing, the corpus has been well cleaned while the content is fully remained with all the parsed unicode text, and the label is remained only if it’s included in the nine main subject.- It tokenize each poetry with removing stop words in English and punctuation. In a nutshell, the content of each poetry is replaced by a sentence which contains each “meaningful” words. **What’s the most important, after this type of tokenization, the content of a poetry is a list of unordered words. When implementing deep networks, please obey the cleaning method issued in paper and remember to remain the logical relations between the words**- After tokenization, if any modification in preprocess is added, please save the modification to the json file of the save path. you could control the function with setting the boolean parameters as true or false.2. train.py- self.split() could be used to split the whole dataset with train/valid/test set.- self.encoder_binary() encodes multiple labels to a binary label(+1/-1) for each subject. By the way, one-hot encode could be used if you’d like to directly implement this multi-label problem.- self.train_svm provides a demo which you could refer or edit as you wish.        + Pipeline is a useful class for you to extract feature with BoW features and employ a classifier.        + I first employ CountVectorizer() to 3. eval.py- After training and inference(predicting), it would be convenient to decouple the evaluation part  off the training part. Thanks to sklearn.metrics, it provides plenty of methods and values you could use to evaluate the performance of the whole model.- As a tip, the model itself just separately treat each subject as a binary-labeled classifier. If we treat it as a multi-labeled issue, there are some good metrics like Hamming loss or macro-average, have a try!TODO List:* try to optimize the feature extraction function, where I could employ feature selection to reduce dimensions* Update CUDA 9.0 to support Tesla P80Hard List:* implement CNN model on text classification : softmax ? or the same binary-encoding on the whole model* metrics? each metric for each subject ? some complex metric for all subjectsSong List:* 一个人睡：魏晨* Way back home：SHAUN