# Poetry Classification## Dependencies+ python version >= 3.6.6+ All required dependencies are clearly listed with their versions following:> pip install -r requirements.txt+ tensorflow-gpu is highly recommended if you would like to reproduce the results on textcnn. Before installing the package with pip, CUDA 9.0, cuDNN and configuration requirements vary due to different types of GPU. Please check the following reference document:> https://gist.github.com/zhanwenchen/e520767a409325d9961072f666815bb8## Instruction> python preprocess.pyThis command will clean the original corpus, such as removal of stopwords, tokenization, and etc. In order to simplify the whole process, we don’t provide any argument outside. Nevertheless, it’s highly recommended that you could adjust boolean parameters inside to generate the required corpus as you wish.> python nb.py && python svm.py && python logreg.py && python mlp.py && python gbt.py- [x] Naive Bayes : nb.py- [x] Support Vector Machine : svm.py- [x] Logistic Regression: logreg.py- [x] Multilayer Perceptron : mlp.py- [x] Gradient Boosting Tree : gbt.pyThese command will activate each classifier and automatically dump evaluation report and result matrix into different folders.There are some tips:+ Always run only one classifier at one time, each is time-consuming especially when you'd like to employ lda as a feature+ Take care especially when you’d like to apply lda model into practice. It’ll take at least one hour to generate the whole result even though 4 CPU parallely run  the same process.+ gbt_param.json is provided. Modification is welcomed.> cd ./textcnn  > mkdir data  > python dataset_gen.py  Preprocess and generate the whole dataset> python train.py --help>>  -h, --help            show this help message and exit      --embedding_dim EMBEDDING_DIM, Dimensionality of character embedding (default: 128)      --filter_sizes FILTER_SIZES, Comma-separated filter sizes (default: '3,4,5')      --num_filters NUM_FILTERS, Number of filters per filter size (default: 128)      --l2_reg_lambda L2_REG_LAMBDA, L2 regularizaion lambda (default: 0.0)      --dropout_keep_prob DROPOUT_KEEP_PROB, Dropout keep probability (default: 0.5)      --batch_size BATCH_SIZE, Batch Size (default: 64)      --num_epochs NUM_EPOCHS, Number of training epochs (default: 100)      --evaluate_every EVALUATE_EVERY, Evaluate model on dev set after this many steps  (default: 100)      --checkpoint_every CHECKPOINT_EVERY, Save model after this many steps (default: 100)      --allow_soft_placement ALLOW_SOFT_PLACEMENT, Allow device soft device placement      --noallow_soft_placement      --log_device_placement LOG_DEVICE_PLACEMENT, Log placement of ops on devices      --nolog_device_placement  > python train.pyBefore training and evaluating, please modify the type name **parameter**, typeName into the required one.If you’d like to fine-tune the whole model, you could use the command below to set each hyper-parameters. Especially batch_size with 64/128 is highly recommended. If tensorflow-gpu is correctly installed, please change **tf.device('/cpu:0')** into **tf.device('/gpu:0')**> python eval.py --eval_train True --checkpoint_dir="./runs/{ACTUAL_NUMBER}/checkpoints/"## Code Logic1. preprocess.py- Before preprocessing, the corpus has been well cleaned while the content is fully remained with all the parsed unicode text, and the label is remained only if it’s included in the nine main subject.- It tokenize each poetry with removing stop words in English and punctuation. In a nutshell, the content of each poetry is replaced by a sentence which contains each “meaningful” words. **What’s the most important, after this type of tokenization, the content of a poetry is a list of unordered words. When implementing deep networks, please obey the cleaning method issued in paper and remember to remain the logical relations between the words**- After tokenization, if any modification in preprocess is added, please save the modification to the json file of the save path. you could control the function with setting the boolean parameters as true or false.- Besides, it splits the generated dataset into train/valid/test collections with a fixed percentage.2. feature.py- Combine all the functions which are relevant to feature extraction- You'd better expand Feature(trained, args**) when you'd like to fine-tune parameters during feature selection.> For example, def __init__(self, trained=True, lda_n_component):- word2vec has been well trained on the small corpus (without any stopwords), w2vmodel is available for other training methods3. classifier.py- Each classifier could be regarded as a binary classifier after label encoded.- self.train() provides the usage which you could refer or edit as you wish.> Pipeline is a useful class for you to extract feature with BoW features and employ a classifier.> The pipeline combines feature extraction and classification4. eval.py- After training and inference(predicting), it would be convenient to decouple the evaluation part  off the training part. Thanks to sklearn.metrics, it provides plenty of methods and values you could use to evaluate the performance of the whole model.- As a tip, the model itself just separately treat each subject as a binary-labeled classifier. If we treat it as a multi-labeled issue, there are some good metrics like Hamming loss or macro-average, have a try!